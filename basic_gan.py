# -*- coding: utf-8 -*-
"""Basic GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10vrOIDLzPg4RI7qumKHaEfJdpjQubAzK
"""

#import the libraries
import torch,pdb
import torch.nn as nn
from torch.utils.data import DataLoader
from torch import nn
from torchvision import transforms
from torchvision.datasets import MNIST
from torchvision.utils import make_grid
from tqdm.auto import tqdm
import matplotlib.pyplot as plt

#visualization function
def show(tensor,ch=1,size=(28,28),num=16):
  #tensor:128 X
  data=tensor.detach().cpu().view(-1,ch,*size) # 128 x 1 x 28 x 28
  grid= make_grid(data[:num],nrow=4).permute(1,2,0) #1 x 28 x 28 = 28 x 28 x 1
  plt.imshow(grid)
  plt.show()

# setup of the main parameters and hyperparameters
epochs=500
cur_step=0
info_step=300
mean_gen_loss=0
mean_disc_loss=0

z_dim=64
lr = 0.00001 #learning rate
loss_func=nn.BCEWithLogitsLoss()

bs=128   #batch size
device='cuda'

dataloader =DataLoader(MNIST('.',download=True,transform=transforms.ToTensor()),shuffle=True,batch_size=bs)

# declare our models

# generator of each block
def genBlock(inp,out): # input and output size
  return nn.Sequential(
    nn.Linear(inp,out),
    nn.BatchNorm1d(out),
    nn.ReLU(inplace=True)
  )

class Generator(nn.Module):
  def __init__(self,z_dim,i_dim=784,h_dim=128):
    super().__init__()
    self.gen=nn.Sequential(
        genBlock(z_dim,h_dim), #64, 128
        genBlock(h_dim,h_dim*2), #128 ,256
        genBlock(h_dim*2,h_dim*4), #256 ,512
        genBlock(h_dim*4,h_dim*8), #512,1024
        nn.Linear(h_dim*8,i_dim),#1024,784
        nn.Sigmoid()
    )

  def forward(self,noise):
      return self.gen(noise)

def gen_noise(number,z_dim):
  return torch.randn(number,z_dim).to(device)



# Discriminator

def discBlock(inp,out):
  return nn.Sequential(
      nn.Linear(inp,out),
      nn.LeakyReLU(0,2)
  )

class Discrimainator(nn.Module):
  def __init__(self,i_dim=784,h_dim=256):
    super().__init__()
    self.disc=nn.Sequential(
        discBlock(i_dim,h_dim*4), #784,1024
        discBlock(h_dim*4,h_dim*2), #1024,512
        discBlock(h_dim*2,h_dim),  #512,256
        nn.Linear(h_dim,1) #256,1
    )

  def forward(self,image):
      return self.disc(image)

gen=Generator(z_dim).to(device)
gen_opt=torch.optim.Adam(gen.parameters(),lr=lr)
disc=Discrimainator().to(device)
disc_opt=torch.optim.Adam(disc.parameters(),lr=lr)

gen

disc

x,y=next(iter(dataloader))
print(x.shape,y.shape)
print(y[:10])

noise=gen_noise(bs,z_dim)
fake=gen(noise)
show(fake)

# here what you are seeing here is the initial output of passing the noise through the generator.
#because the generator did not begin to learn,it produce a very noise output
#now we go to try to tain the network to produce the images of the numbers

# calculating the loss

# generator loss

def calc_gen_loss(loss_func,gen,disc,number,z_dim):
  noise=gen_noise(number,z_dim)
  fake=gen(noise)
  pred=disc(fake)
  targets=torch.ones_like(pred)
  gen_loss=loss_func(pred,targets)
  return gen_loss

def calc_disc_loss(loss_func,gen,disc,number,real,z_dim):
  noise=gen_noise(number,z_dim)
  fake=gen(noise)
  disc_fake=disc(fake.detach())
  disc_fake_target=torch.zeros_like(disc_fake)
  disc_fake_loss=loss_func(disc_fake,disc_fake_target)

  disc_real=disc(real)
  disc_real_target=torch.ones_like(disc_real)
  disc_real_loss=loss_func(disc_real,disc_real_target)

  disc_loss=(disc_fake_loss+disc_real_loss)/2

  return disc_loss

###60000/128 = 468.75 =469 steps in each epoch
### each step is going to prosess 128 images = size of the batchj (except the last step)

for epoch in range(epochs):
  for real, _ in tqdm(dataloader):
    ### discriminator
    disc_opt.zero_grad()

    cur_bs=len(real) #real : 128 X 1 X 28 X 28
    real=real.view(cur_bs,-1) #128 X 784
    real=real.to(device)

    disc_loss=calc_disc_loss(loss_func,gen,disc,cur_bs,real,z_dim)

    disc_loss.backward(retain_graph=True)
    disc_opt.step()

    ##generator

    gen_opt.zero_grad()
    gen_loss = calc_gen_loss(loss_func,gen,disc,cur_bs,z_dim)
    gen_loss.backward(retain_graph=True)
    gen_opt.step()

    ### visualization & stats

    mean_disc_loss+=disc_loss.item()/info_step
    mean_gen_loss+=gen_loss.item()/info_step

    if cur_step % info_step == 0 and cur_step>0:
      fake_noise = gen_noise(cur_bs,z_dim)
      fake=gen(fake_noise)
      show(fake)
      show(real)
      print(f"{epoch}: step {cur_step} / Gen loss : {mean_gen_loss} /disc_loss: {mean_disc_loss} ")
      mean_gen_loss ,mean_disc_loss=0,0

    cur_step+=1

